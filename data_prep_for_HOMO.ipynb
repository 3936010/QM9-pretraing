{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GINConv\n",
    "from torch.nn import Linear, Sequential, BatchNorm1d, ReLU\n",
    "import torch.nn.functional as Fun\n",
    "\n",
    "from torch_geometric.datasets import QM9\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2359210, 11], edge_index=[2, 4883516], edge_attr=[4883516, 4], y=[130831, 19], pos=[2359210, 3], z=[2359210], smiles=[130831], name=[130831], idx=[130831])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qm9 = QM9(root=\"/home/sardorbek/MyResearch/data_prep/data/QM9\")\n",
    "qm9.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_target = pd.DataFrame(qm9.data.y.numpy())\n",
    "y_target = y_target[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "qm9.data.y = torch.Tensor(y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sardorbek/.local/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/home/sardorbek/.local/lib/python3.10/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "qm9 = qm9.shuffle()\n",
    "\n",
    "# data split\n",
    "data_size = 3000\n",
    "train_index = int(data_size*0.8)\n",
    "test_index = train_index + int(data_size*0.1)\n",
    "val_index = test_index + int(data_size*0.1)\n",
    "\n",
    "# normalizing the data\n",
    "data_mean = qm9.data.y[0:train_index].mean()\n",
    "data_std = qm9.data.y[0:train_index].std()\n",
    "\n",
    "qm9.data.y = (qm9.data.y - data_mean)/data_std\n",
    "\n",
    "# dataset into DataLoader\n",
    "train_loader = DataLoader(qm9[0:train_index], batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(qm9[train_index:test_index], batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(qm9[test_index:val_index], batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qm9.data.y[:, 3] [this is homo energy target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2359210, 11], edge_index=[2, 4883516], edge_attr=[4883516, 4], y=[130831, 19], pos=[2359210, 3], z=[2359210], smiles=[130831], name=[130831], idx=[130831])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qm9.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model [GNN] architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HOMOnet(torch.nn.Module):\n",
    "    def __init__(self, dim_h):\n",
    "        super(HOMOnet, self).__init__()\n",
    "        self.conv1 = GINConv(\n",
    "            Sequential(\n",
    "                Linear(11, dim_h), BatchNorm1d(dim_h), ReLU(), Linear(dim_h, dim_h), ReLU()\n",
    "            )\n",
    "        )\n",
    "        self.conv2 = GINConv(\n",
    "            Sequential(\n",
    "                Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(), Linear(dim_h, dim_h), ReLU()\n",
    "            )\n",
    "        )\n",
    "        self.conv3 = GINConv(\n",
    "            Sequential(\n",
    "                Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(), Linear(dim_h, dim_h), ReLU()\n",
    "            )\n",
    "        )\n",
    "        self.lin1 = Linear(dim_h, dim_h)\n",
    "        self.lin2 = Linear(dim_h, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        edge_index= data.edge_index\n",
    "        batch = data.batch\n",
    "\n",
    "        # Node Embaddings\n",
    "        h = self.lin1()\n",
    "        h = h.relu()\n",
    "        h = Fun.dropout(h, p=0.5, training=self.training)\n",
    "        h = self.lin2(h)\n",
    "\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(loader, model, loss, optimizer):\n",
    "    \"\"\"Training one epoch\n",
    "\n",
    "    Args:\n",
    "        loader(DataLoader): training data devidev into batchs\n",
    "        model(nn.Module): GNN model to train on\n",
    "        loss: loss function to use during training\n",
    "        optimizer(torch.optim): optimizer for training\n",
    "\n",
    "    Returns:\n",
    "        training loss\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    current_loss = 0\n",
    "    for d in loader:\n",
    "        optimizer.zero_grad()\n",
    "        d.x = d.x.float()\n",
    "\n",
    "        out = model(d)\n",
    "\n",
    "        l = loss(out, torch.reshape(d.y, (len(d.y), 1)))\n",
    "        current_loss += 1/len(loader)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "    return current_loss, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation\n",
    "def validation(loader, model, loss):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    for d in loader:\n",
    "        out = model(d)\n",
    "        l = loss(out, torch.reshape(d.y, (len(d.y), 1)))\n",
    "        val_loss += 1/len(loader)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "\n",
    "def testing(loader, model):\n",
    "    loss = torch.nn.MSELoss()\n",
    "    test_loss = 0\n",
    "    test_target = np.empty((0))\n",
    "    test_y_target = np.empty((0))\n",
    "    for d in loader:\n",
    "        out = model(d)\n",
    "        l = loss(out, torch.reshape(d.y, (len(d.y), 1)))\n",
    "        test_loss += 1/len(loader)\n",
    "\n",
    "        # save prediction vs ground_truth values for plotting\n",
    "        test_target = np.concatenate((test_target, out.detach().numpy()[:, 0]))\n",
    "        test_y_target = np.concatenate((test_y_target, d.y.detach().numpy()))\n",
    "    return test_loss, test_target, test_y_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epochs(epochs, model, train_loader, val_loader, path):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    loss = torch.nn.MSELoss()\n",
    "\n",
    "    train_target = np.empty((0))\n",
    "    train_y_target = np.empty((0))\n",
    "    train_loss = np.empty(epochs)\n",
    "    val_loss = np.empty(epochs)\n",
    "    best_loss = math.inf\n",
    "\n",
    "    for e in range(epochs):\n",
    "        epoch_loss, model = training(train_loader, model, loss, optimizer)\n",
    "        v_loss = validation(val_loss, model, loss)\n",
    "        if v_loss < best_loss:\n",
    "            torch.save(model.state_dict(), path)\n",
    "        for d in train_loader:\n",
    "            out = model(d)\n",
    "            if e == epochs - 1:\n",
    "                # record truly vs predicted value\n",
    "                train_target = np.concatenate((train_target, out.detach().numpy()[:, 0]))\n",
    "                train_y_target = np.concatenate((train_y_target, d.y.detach().numpy()))\n",
    "        \n",
    "        train_loss[epochs] = epoch_loss.detach().numpy()\n",
    "        val_loss[epochs] = v_loss.detach().numpy()\n",
    "\n",
    "        # print current train and val loss\n",
    "        if e % 2 == 0:\n",
    "            print(\n",
    "                \"Epoch: \",\n",
    "                str(e)\n",
    "                + \", Train Loss: \"\n",
    "                + str(epoch_loss.item())\n",
    "                + \", Val Loss: \"\n",
    "                + str(v_loss.item())\n",
    "            )\n",
    "    return train_loss, val_loss, train_target, train_y_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HOMOnet(\n",
       "  (conv1): GINConv(nn=Sequential(\n",
       "    (0): Linear(in_features=11, out_features=64, bias=True)\n",
       "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (4): ReLU()\n",
       "  ))\n",
       "  (conv2): GINConv(nn=Sequential(\n",
       "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (4): ReLU()\n",
       "  ))\n",
       "  (conv3): GINConv(nn=Sequential(\n",
       "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (4): ReLU()\n",
       "  ))\n",
       "  (lin1): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (lin2): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = HOMOnet(dim_h=64)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epoch Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Linear.forward() missing 1 required positional argument: 'input'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[101], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_loss, val_loss, train_target, train_y_target \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epochs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/sardorbek/MyResearch/TainHOMOnet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[97], line 12\u001b[0m, in \u001b[0;36mtrain_epochs\u001b[0;34m(epochs, model, train_loader, val_loader, path)\u001b[0m\n\u001b[1;32m      9\u001b[0m best_loss \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39minf\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 12\u001b[0m     epoch_loss, model \u001b[38;5;241m=\u001b[39m \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     v_loss \u001b[38;5;241m=\u001b[39m validation(val_loss, model, loss)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m v_loss \u001b[38;5;241m<\u001b[39m best_loss:\n",
      "Cell \u001b[0;32mIn[93], line 20\u001b[0m, in \u001b[0;36mtraining\u001b[0;34m(loader, model, loss, optimizer)\u001b[0m\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     18\u001b[0m d\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m d\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m---> 20\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m l \u001b[38;5;241m=\u001b[39m loss(out, torch\u001b[38;5;241m.\u001b[39mreshape(d\u001b[38;5;241m.\u001b[39my, (\u001b[38;5;28mlen\u001b[39m(d\u001b[38;5;241m.\u001b[39my), \u001b[38;5;241m1\u001b[39m)))\n\u001b[1;32m     23\u001b[0m current_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(loader)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[92], line 28\u001b[0m, in \u001b[0;36mHOMOnet.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     25\u001b[0m batch \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mbatch\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Node Embaddings\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlin1\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m h \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mrelu()\n\u001b[1;32m     30\u001b[0m h \u001b[38;5;241m=\u001b[39m Fun\u001b[38;5;241m.\u001b[39mdropout(h, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: Linear.forward() missing 1 required positional argument: 'input'"
     ]
    }
   ],
   "source": [
    "train_loss, val_loss, train_target, train_y_target = train_epochs(\n",
    "    epochs=10, model= model, train_loader=train_loader, val_loader=test_loader, path=\"/home/sardorbek/MyResearch/TainHOMOnet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
